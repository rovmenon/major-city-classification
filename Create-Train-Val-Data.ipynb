{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf2813-7285-46f6-b38a-8528a56ee92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import axes_grid1\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3a008-1b04-445b-8063-159d03f50f75",
   "metadata": {},
   "source": [
    "first download dataset from https://www.kaggle.com/datasets/amaralibey/gsv-cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976456c2-8878-4ee5-99b4-040b6ef45e85",
   "metadata": {},
   "source": [
    "# Load in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73240620-1d1e-4f19-bc44-6ff84cfef0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "path_to_data = \"../GSVData/Dataframes\"\n",
    "for csv in os.listdir(path_to_data):\n",
    "    if df.empty:\n",
    "        df = pd.read_csv(os.path.join(path_to_data,csv))\n",
    "    else:\n",
    "        temp = pd.read_csv(os.path.join(path_to_data, csv))\n",
    "        df = pd.concat([df, temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb6ad3-07fe-4b7b-9cd9-3334f84e2dfa",
   "metadata": {},
   "source": [
    "take only the placeids with >= 4 photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9df88-46ae-45bd-848c-5fb99da30bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gte_4 = df[df.groupby(['city_id','place_id'])['place_id'].transform('size') >= 4]\n",
    "unique_ids = gte_4[['city_id', 'place_id']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d665e-974b-4320-bad5-53a3c9bbc5fa",
   "metadata": {},
   "source": [
    "# Randomly split into train and val (.75 place id images in train, .25 in val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44eee04-e2a9-4360-8973-11c02fd0ca65",
   "metadata": {},
   "source": [
    "don't run this cell if you want to replicate our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1443f5d-62fa-4542-9956-a866f253e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=['place_id', 'year', 'month', 'northdeg', 'city_id', 'lat', 'lon',\n",
    "       'panoid'])\n",
    "test_df = pd.DataFrame(columns=['place_id', 'year', 'month', 'northdeg', 'city_id', 'lat', 'lon',\n",
    "       'panoid'])\n",
    "for city, place in tqdm(unique_ids.index):\n",
    "    temp_train = gte_4[(gte_4['city_id'] == city) & (gte_4['place_id'] == place)].sample(frac=.75)\n",
    "    temp_test = pd.concat([gte_4[(gte_4['city_id'] == city) & (gte_4['place_id'] == place)],temp_train]).drop_duplicates(keep=False)\n",
    "    train_df = pd.concat([train_df, temp_train])\n",
    "    test_df = pd.concat([test_df, temp_test])\n",
    "\n",
    "train_df.to_csv(\"train_df.csv\")\n",
    "test_df.to_csv(\"test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d0e1a-73fe-49f2-ae55-75e307e49067",
   "metadata": {},
   "source": [
    "run the following after using our dataframes for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdca41-a365-4aa5-8c28-dafbdb7091a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv(\"train_df.csv\")\n",
    "#test_df = pd.read_csv(\"test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca300f-bc59-4bff-b146-a06e1d09c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_moved = []\n",
    "\n",
    "for idx, img in test_df.iterrows():\n",
    "    path = img['city_id'] + '_' + str(img['place_id']).zfill(7) + '_' + str(img['year']) \\\n",
    "    + '_' + str(img['month']).zfill(2) + '_' + str(img['northdeg']).zfill(3) + '_' + str(img['lat']) + '_' \\\n",
    "    + str(img['lon']) + '_' + img['panoid'] + '.jpg'\n",
    "    new_path = os.path.join(\"../GSVData\", \"Test\", img['city_id'], path)\n",
    "    old_path = os.path.join(\"../GSVData\", \"Images\", img['city_id'], path)\n",
    "\n",
    "    if os.path.exists(old_path):\n",
    "        shutil.move(old_path, new_path)\n",
    "    else:\n",
    "        not_moved.append(old_path)\n",
    "\n",
    "os.rename('..GSVData/Images', 'GSVData/Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98727b91-d309-4654-a2ad-e1e576021575",
   "metadata": {},
   "source": [
    "# Creating Custom Batches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2f227-960e-4efc-a2b6-2e53fd82696c",
   "metadata": {},
   "source": [
    "The purpose of the following code is to be able to create custom batches where images from the same place id are in the same batch. We want to do this to give our CNN the best possible chance of extracting relevant feature for each place in the same batch. As of now, the code does not completely work, we will revisit this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63694d05-6694-4448-9dd3-b2d512f0ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pathnames(img):\n",
    "    path = img['city_id'] + '_' + str(img['place_id']).zfill(7) + '_' + str(img['year']) \\\n",
    "    + '_' + str(img['month']).zfill(2) + '_' + str(img['northdeg']).zfill(3) + '_' + str(img['lat']) + '_' \\\n",
    "    + str(img['lon']) + '_' + img['panoid'] + '.jpg'\n",
    "    return os.path.join(\"../GSVData\", \"Train\", img['city_id'], path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79903f1b-762f-49f3-ad9d-db879fb62cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['path'] = train_df.apply(lambda x: create_pathnames(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bca407-1b98-4cde-86df-d35afd3d0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "train_df['encoded_city'] = label_encoder.fit_transform(train_df['city_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80303051-e4fc-4b44-9349-8e422b4fec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419abd94-4456-445d-aa75-fde399ba8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = train_df.groupby(['city_id', 'place_id'], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc471777-c4b1-4af6-a1bd-c507f0077297",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "batchesn = []\n",
    "batches = []\n",
    "batch_size = 256\n",
    "curr_batch = 0\n",
    "batchn = []\n",
    "batch = []\n",
    "batch_lens = []\n",
    "labels = []\n",
    "label_b = []\n",
    "for group_keys, group_data in train_grouped:\n",
    "    if curr_batch + group_data.shape[0] <= batch_size:\n",
    "        batchn.append(group_keys)\n",
    "        batch += list(group_data['path'])\n",
    "        label_b += list(group_data['encoded_city'])\n",
    "        curr_batch += group_data.shape[0]\n",
    "    else:\n",
    "        batch_lens.append(curr_batch)\n",
    "        batchesn.append(batchn)\n",
    "        batches.append(batch)\n",
    "        labels.append(label_b)\n",
    "        batchn = [group_keys]\n",
    "        batch = list(group_data['path'])\n",
    "        label_b = list(group_data['encoded_city'])\n",
    "        curr_batch = group_data.shape[0]\n",
    "    total += group_data.shape[0]\n",
    "batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac19406-f7e1-4755-8480-e4e448f20357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode_image(image_path):\n",
    "    # Read the image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode the image\n",
    "    image = tf.image.decode_jpeg(image, channels=3) \n",
    "    # Preprocess the image (resize, normalize, etc.)\n",
    "    image = preprocess_image(image) \n",
    "    return image\n",
    "    \n",
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, [128, 96])\n",
    "    image = image / 255.0  \n",
    "    return image\n",
    "\n",
    "# Function to load a batch of images\n",
    "def load_batch(batch_paths, labels):\n",
    "    # Create a dataset from the list of paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((batch_paths, labels))\n",
    "    # Map the dataset to read and decode images\n",
    "    dataset = dataset.map(lambda x, y: (read_and_decode_image(x), y)) #, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# List to hold datasets for each batch\n",
    "batch_datasets = []\n",
    "\n",
    "# Load each batch of images into a TensorFlow dataset\n",
    "for batch_paths, l in tqdm(zip(batches, labels)):\n",
    "    batch_dataset = load_batch(batch_paths, l)\n",
    "    ts = [tensor for tensor, _ in batch_dataset]\n",
    "    ls = [_ for tensor, _ in batch_dataset]\n",
    "    batch_dataset = (tf.stack(ts, axis=0), tf.stack(ls, axis=0))\n",
    "    batch_datasets.append(batch_dataset)\n",
    "    del batch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3085996-f383-48dc-8ebb-bcc689b7e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = batch_datasets[0]\n",
    "for dataset in batch_datasets[1:]:\n",
    "    combined_dataset = combined_dataset.concatenate(dataset)\n",
    "\n",
    "# Convert the combined dataset to a MapDataset\n",
    "map_dataset = combined_dataset.map(lambda x, y: (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab9b97-241f-4f86-808e-2bd47a1de60e",
   "metadata": {},
   "source": [
    "Because the custom batching is not yet functional, we will use tensorflows built in dataloader tf.keras.preprocessing.image_dataset_from_directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
